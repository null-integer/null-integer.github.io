
//Project Database
let projectDB = [
    // {projectSection:"",projectName: "", projectDescription:"", projectImages:[]},
    {projectSection:"Algorithms & Data Structures",projectName: "Trend Tracker", projectDescription:"The trend tracker assignment consists of constructing a vector-based data structure that tracks information about a collection of hashtags. The TrendTracker class has eight class methods: insert(string ht), size(), tweeted(string ht), popularity(string ht), top_trend(), top_three_thrends(vector&lt;string&gt; &amp;T), remove(string ht), top_k_trends(vector&lt;string&gt; &amp;T, int k).<br/><br/>&#x2022;Insert(string ht) - The insert method iterates the vector, looking for ht. If ht is found, the method ends early, doing nothing to the current vector. On the other hand, if ht is found, an empty entry is added to the vector. This method runs in O(n) time.<br/><br/>&#x2022;Size() - The size method returns the number of hashtags in the trend tracker. This utilizes the vector size method to run in constant time.<br/><br/>&#x2022;Tweeted(string ht) - The tweeted function acts as an incrementer for the trends. Similar to the insert function, the vector is traversed in linear time, looking for the string to modify.<br/><br/>&#x2022;Popularity(string ht) - This method returns the popularity integer belonging to the hashtag.<br/><br/>&#x2022;Top_trend() - The trend with the highest popularity integer is returned.<br/><br/>&#x2022;Top_three_trends(vector&lt;string&gt; &amp;T) - The top 3 trends with the highest popularity integers are stored in the T vector.<br/><br/>&#x2022;Remove(string ht) - Removes the given hashtag from the trend tracker vector.<br/><br/>&#x2022;Top_k_trends(vector&lt;string&gt; &amp;T, int k) - The top k trends with the highest popularity integers are stored in the T vector.<br/>", projectImages:["trendtracker.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Trend Tracker 2", projectDescription:"Although the previous TrendTracker is efficient enough for a couple thousand hashtags, it slows down as the size of the vector increases. This project uses an efficient two-vector-based data structure to speed up the base TrendTracker class. This faster class contains the same methods but using the help of the additional S integer vector, the running times are decreased. The S integer vector keeps track of the indices of the top trends and by populating the trend tracker in increasing order, a binary search can be performed to fetch a specific string in logarithmic time. Tweeted and Popularity can now be performed in O(log(n)); while top_three_trends is now a constant time function. Running it against 1.5 million hashtags, the program still has speedy execution times.", projectImages:["trendtracker2.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Linked Lists", projectDescription:"In this project, I experimented with linked lists to implement more complex data structure (stacks, queues, &amp; priority queues).<br/><br/>&#x2022;Stack - This class contained 5 methods: stack, empty, push, pop, and insertAt. The stack method served as a destructor to make sure there was no memory leak. It looped the stack in linear time, popping the elements until it was empty. The empty method returned a boolean indicating if the list was empty; this is done in constant time. To push or pop in a stack, the new element is appended or removed from the head of the list, so this method runs in constant time as well. To insert an element in a specific index, we must iterate the linked list until we find the specified element and break the link in the current sequence to insert the new element in the middle; at worst, the insertAt function takes linear time.<br/><br/>&#x2022;Queue - The queue class works similar to stack. However, in order to achieve similar constant times, the queue requires storing to pointers, one pointing to the head and one pointing to the rear. To enqueue, we append to the rear, and to dequeue, we remove from the head.<br/><br/>&#x2022;PriorityQueue - The PriorityQueue class works similar to Queue; however it extracts the minimum element to ensure fast access times for desired elements.", projectImages:["stack.png", "queue.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Auto Completer", projectDescription:"The autocompleter project simulates that of a smartphone's autocompleter feature. Given a substring, it provides the top three completions with the most frequency. This class uses an AVL binary search tree to provide speedy results. The search tree rebalances itself to insert in logarithmic time. Insertion works by firstly recursively inserting the new node; after that, the tree is rebalanced by performing the necessary rotations to keep the AVL rule always applicable (height difference between a right and left subtree cannot exceed one node).", projectImages:["autocompleter.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Auto Completer 2", projectDescription:"The previous autocompleter uses a balanced binary search tree. This leads to a O(log(n)) worst-case run time. This is a good time; however in modern times, this type of latency is not acceptable. In this project, I speed up the autocompleter class by using a different abstract data structure. To achieve a constant worst-case running time, I employ a trie data structure.<br/><br/>&#x2022;Insert - To insert a word in constant time, relative to the amount of saved words, we follow the trie, letter by letter, until we reach the end, marking the end as a valid word.<br/><br/>&#x2022;Completions - To recover valid completions in constant time, we follow the trie path and append possible completions to a vector when a node is marked as a valid word.", projectImages:["trie.jpg"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Priority Queue", projectDescription:"The purpose of this project was to implement a Min Priority Queue to use in later projects. All major operations, push, pop, and decrease_key, in this class run in O(log(n)) time. This Priority Queue uses a heap, represented by a vector of string to integer map, and an unordered map, mapping the string with the index it belongs to in the heap vector.<br/><br/>&#x2022;All operations work in a similar matter. To push, pop, or decrease priority, a new pair is appended to the heap. Since the order of priorities in the updated heap might not be in the correct order, the priority queue is now compromised. The heap is therefore reordered by taking advantage of the sorted nature of the vector. By using a binary search approach, the elements in the heap are swapped until the correct order is ensured. This leads to O(log(n)) operation times.", projectImages:["priorityQueue.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Maze", projectDescription:"The purpose of this project was to implement a breadth-first search maze-solving algorithm. This program takes ASCII art format and returns a possible solution if possible. To traverse the ASCII map using breadth-first search, I had to represent it as a graph. Each character in the maze is represented as a vertex with edges connecting to neighboring vertices. The program firstly reads the input and converts the map into a graph using the previous rules. Once the graph is constructed, the program tries to find a path from the entrance of the maze to the exit by creating a queue and keeping track of visited nodes. The program keeps marking unvisited nodes until the exit is found or the queue is empty. To ensure the program recalls the correct path, it leaves breadcrumbs at every stage of how it got there, once done, it can backtrack from the exit to the entrance. Using BFS also ensures the path is the shortest possible.", projectImages:["maze.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Maze 2", projectDescription:"The goal of this project was to improve upon the previous maze program. The downside of the breadth-first search algorithm is its weakness against weighted edges. If some path has a cost to get there, the algorithm will be oblivious to this. In this new maze problem, the concept of 'portals' is introduced. Here, any blank space is free of charge as the last problem; however, throughout the maze there are digits that represent portals. If the path goes over the portal, it will continue on the matching digit at that cost. For example, if the user steps over a 5, it will transport to the other 5 in the maze at a cost of 5 steps. To do this, breadth-first search is replaced with Djikstra's algorithm for shortest paths. Converting the map to a graph follows a similar process but requires to give weights to each edge. Following Djikstra's, each vertex is initialized with a large sentinel value. After that, the minimum vertex is extracted, this can be done by using the previous minimum priority queue project. Once the minimum vertex is extracted, each adjacent node is relaxed. This is done until the priority queue is empty.", projectImages:["maze2.png"]},
    {projectSection:"Algorithms & Data Structures",projectName: "Arbitrage", projectDescription:"Arbitrage is a well known problem in computer science; it is the process of using exchange rate discrepancies to take a currency and transform it into more than one unit of the same currency. In order to determine if the exchange of a sequence of currencies yields a profit, we can represent currencies and exchange rates as a weighted, directed graph. To do this, we can represent each currency as a vertex with an edge connecting every vertex to every other one. If we can start at one currency and traverse back to its original vertex, we have found a cycle, not necessarily a profitable path however. To start, we initialize a graph, G, with n vertices with n(n-1) edges connecting them. We can then use the Bellman-Ford algorithm to find if there exists a cycle. However, a few problems arise while using this algorithm. First of all, Bellman-Ford uses the addition of individual edge weights to find the shortest distance from a source edge, so to represent multiplication as addition we can use the product rule of logarithms, log(XY) = logX + logY. Therefore, the new arbitrage inequality becomes log(R[i1,i2]) + log(R[i2, i3]) ... + log(R[ik-1, ik]) + log(R[ik, i1]) > log(1). However if we use Bellman-Ford to detect a negative weight cycle with these edge weights it will fail since the sum is over log(1) or 0. To flip the inequality we can view it as log(1/R[i1, i2]) + log(1/R[i2, i3]) ... + log(1/R[ik-1, ik]) + log(1/R[ik, i1]) > 0. With these weights, we can successfully detect a negative weight cycle. Therefore each edge of G has weight log(1/R[i,j]); alternatively one can use the equivalent form of -log(R[i,j]). One last problem arises when we try to set the source vertex which is not given since we can choose from any starting currency. We can solve this issue by adding an extra, unidirectional vertex with 0 weight to all vertices and use this as a source. Now, we run Bellman-Ford to find the shortest distances from the source to the other vertices. Lastly, for any edge, (i,j), if the sum of the shortest path to j and the weight of (i,j) is less than the shortest path to i, we found a negative weight cycle, or arbitrage.", projectImages:["arbitrage.jpeg"]},

    {projectSection:"Neural Networks & Deep Learning",projectName: "DQN Boxing & Breakout", projectDescription:"This project served the purpose to experiment with DQN, Deep Q-Network. DQN is a reinforcement learning algorithm which combines deep learning and Q-learning to find the optimal function between actions and expected future rewards.<br/><br/>To test DQN's ability to learn from difficult environments, we can train an agent to play the 1980 Boxing Atari Game. In the Boxing game, two players punch each other; the winner is decided by reaching 100 punches or the most punches landed at the end of 2 minutes. By using the Gym library, the boxing Atari game is easily emulated. The DQN agent, white player, reached a max score of 83 in 450 episodes. Additionally, a DQN was trained to play Breakout, reaching a 313 score.", projectImages:[], projectVideos:["https://www.youtube.com/embed/D4UgwNcJtYw?si=MPRcX87s5-FJUQ3z&amp;start=7","https://www.youtube.com/embed/gq0P9-_OXbg?si=c1j2GbTjcTUDwQjO"]},
    {projectSection:"Neural Networks & Deep Learning",projectName: "MNIST CNN", projectDescription:"MNIST is a classic dataset in the field of machine learning. Optimizing accuracy and decreasing loss is frequently benchmarked with this dataset due to its ubiquity. In this project, the dataset was utilized to train a convolutional neural network and achieve 99.04% accuracy in handwritten digit classification. The process takes advantage of modern techniques such as GPU parallelization and cloud training in order to achieve better results faster. Compared to other methods, this model ranks 21st among thebest results according to the Kaggle database.", projectImages:["MNIST_training_acc.png","MNIST_training_loss.png","MNIST_visualized.png"]},
    {projectSection:"Neural Networks & Deep Learning",projectName: "Binary Classification", projectDescription:"In this project, a convolutional neural network for car image classification was implemented. The dataset was manually collected including two classes, sedan and pickup. To achieve an 88.5% classification accuracy, hyperparameters included: 85 epochs, 32 batch size, 40% random horizontal flip, and 40% random perspective of 50% distortion. To achieve a 97% classification accuracy, transfer learning was used; the efficientnet_b1 model was transferred to predict on this specific dataset.", projectImages:["pickup1.jpg","sedan1.jpg"]},
    {projectSection:"Neural Networks & Deep Learning",projectName: "Lunar Lander", projectDescription:"This project develops a reinforcement learning model to deploy the Lunar Lander environment in the OpenAI Gym library. To achieve a smooth landing, temporal difference actor-critic learning methods were used. In this program, there are two networks, one which decides possible actions (actor) and one which judges these actions and provides feedback on how to improve (critic). This learning method reached a score of 262.6 in 9k episodes with the following parameters: 0.0002 learning rate, 0.98 gama, and 10 rollouts. Additionally, a headless mode is implemented to run the testing in the background and save the results in JSON and mp4 files.", projectImages:[], projectVideos:["https://www.youtube.com/embed/oFQCnQGj7mE?si=8G6ie67GR2GPtqBI&amp;start=13"]},

    {projectSection:"Web Development",projectName: "Server Side Shopping", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "Working with a Database", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "Session & Security", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "Javascript & DOM", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "AJAX", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "React", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "ORM", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "URL Saver Script", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "Playlist Sidebar Script", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Web Development",projectName: "AutoScroller Script", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},

    {projectSection:"Mobile Development",projectName: "TrueVault - Credential Manager", projectDescription:"This project, TrueVault, aimed to create a credential manager following an Agile methodology throughout the process. With modern authentication, one dilemma is very persistent, the compromise between convenience and security. This project tries to improve service authentication to reduce the compromise between convenience and security. Compared to existing password managing software, TrueVault encryption and decryption also occurs locally so sensitive data does not travel over the internet when stored in the server. Additionally, TrueVault also uses symmetric AES encryption with a 256 bit key like many other established services. However, this service aims to reduce points of friction between the user and their credentials.<br/><br/>The application was developed using the Flutter framework. For data storage and user management, Google's route was followed and used Firebase's Firestore Database and Authentication. To verify that the system met requirements I followed the convention of dividing it into three sections, widget, unit, and integration tests. In order to ensure working code and that new features do not introduce any regressions, Agile methodology makes use of the aforementioned tests: unit tests, which test functions, methods or classes in isolation; widget tests, which test single widgets that form part of the user interface; and integration tests, which test the app as a whole by simulating user interaction, using a phone emulator to make this possible.", projectImages:["truevault2.png","truevault3.png",]},

    {projectSection:"Data Mining",projectName: "Aprori Algorithm", projectDescription:"Here, a program was written to implement the classical Apriori algorithm to find the most frequent item sets and then generate association rules from these item sets. The Apriori algorithm is used in data mining for finding correlation between items; for example, a grocery store might want to look at market basket data to find items that are frequently bought together. This way, stores are able to arrange aisles or discounts based on this information.<br/><br/>In this project, the 1984 United States Congressional Voting Records data was used. This dataset consists of 435 rows, 16 attributes each, which signify how a republican or democrat congressperson voted on 16 different issues. The program was able to generate correlation on how a certain political party would vote based on other votes. To evaluate each association rule generated, lift can be used, which is the measure that tells us if the probability of voting something a certain way increases or decreases based on their vote on another issue. Lift is calculated by dividing the confidence of the rule by the set difference. All the rules that exhibit lift over 1.0 mean that there is a positive correlation between the two sets, therefore are meaningful rules. Below are some meaningful rules found based on the dataset.<br/><br/>The following parameters were used: MinSupport = 30%, MinConfidence = 90%", projectImages:["apriori.png"]},

    {projectSection:"Computer Architecture",projectName: "Assembly Fibonacci", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Computer Architecture",projectName: "Cuda Vector Sum", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Computer Architecture",projectName: "Hybrid Sorting", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Computer Architecture",projectName: "World Cities", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},

    {projectSection:"Digital Image Processing",projectName: "2D FFT", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Digital Image Processing",projectName: "Laplacian Filtering", projectDescription:"In digital image processing, the Laplacian Filter is used to detect edges in images. When given an image, this high pass filter blocks any low frequencies from the output, leaving only the edges of objects. Mathematically, the Laplacian Filter is described as the matrix [[2,0,2],[0,-8,0],[2,0,2]]. This matrix is convolved on a zero-padded input image and then cropped to get something similar to the images below, where the left image is the input and the right is the output.", projectImages:["input.png","Laplacian-Filter.png"]},

    {projectSection:"Robotics Programming",projectName: "Rotation Matrices", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Robotics Programming",projectName: "Autonomous Mobile Robots", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Robotics Programming",projectName: "PacMan Maze", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},
    {projectSection:"Robotics Programming",projectName: "EKF", projectDescription:"Currently in the process of importing this project to this page.", projectImages:["robot.png"]},

];


function showProject(project){

    document.getElementById("projectName").innerText = project.projectName;

    document.getElementById("implementationImages").innerHTML = "";
    document.getElementById("implementationVideos").innerHTML = "";

    if(project.projectVideos){
        project.projectVideos.forEach((video) =>{

            let iframe = document.createElement('iframe');
            iframe.width = '560';
            iframe.height = '315';
            iframe.src = video;
            iframe.title = project.projectName;
            iframe.frameBorder = "0";
            iframe.allowFullscreen = true;
            document.getElementById("implementationVideos").appendChild(iframe);
        });
    }

    project.projectImages.forEach((image) =>{
        let img = document.createElement('img');
        img.src = "img/"+image;
        img.alt = "Project Screenshot";
        document.getElementById("implementationImages").appendChild(img);
    });

    document.getElementById("implementationDetails").innerHTML = "<h4>"+project.projectDescription+"</h4>";

}

//Add all the projects to their respective menus
function populateMenus(){
    console.log("Menus are populated");
    projectDB.forEach((project) =>{

        //creating a div node for the project
        let projectDiv = document.createElement("div");
        projectDiv.classList.add("project");
        projectDiv.innerHTML = `&#8627;${project.projectName}<br/>`;
        projectDiv.addEventListener("click", function(){showProject(project)}); 
        document.getElementById(project.projectSection).appendChild(projectDiv);
    
    });
}

//Display project amounts
document.getElementsByClassName("projectMenu")[document.getElementsByClassName("projectMenu").length - 1].innerText = `Total Projects: ${projectDB.length}`;
populateMenus();

//Add horizontal line for project division
let submenus = document.getElementsByClassName("projectSubmenu");
Array.from(submenus).forEach((menu)=>{
  menu.appendChild(document.createElement('hr'));
});
showProject(projectDB[0]);